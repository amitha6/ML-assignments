{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2daaa09b",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "087acd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "#from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5531d2",
   "metadata": {},
   "source": [
    "This is a regression problem in which the goal is to use meteorological and\n",
    "other data to predict the burnt area of forest fires in the northeast region of Portugal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22890261",
   "metadata": {},
   "source": [
    "# Separating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "88a8c496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "data_c = pd.read_csv('Abalone_Age_2B.csv')\n",
    "df_c = pd.DataFrame(data_c)\n",
    "#np.random.seed(10)\n",
    "#np.random.shuffle(df_c)\n",
    "l = len(df_c)\n",
    "print(df_c['Age'].max())\n",
    "print(df_c['Age'].min())\n",
    "age = df_c['Age']\n",
    "age = np.array(age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3e3c9",
   "metadata": {},
   "source": [
    "# Making three Age Groups\n",
    "Dividing groups into three classes.  \n",
    "AGE_Group :   \n",
    "2 to 12 is class '0'  \n",
    "13 to 19 is class '1'  \n",
    "20 to 30 is class '2'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "cefdd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "i = 0\n",
    "for i in range(l):\n",
    "    if(age[i] >=2 and age[i]<=12):\n",
    "        c.append(0)\n",
    "    if(age[i] >=13 and age[i]<=19):\n",
    "        c.append(1)\n",
    "    if(age[i] >=20 and age[i]<=30):\n",
    "        c.append(2)\n",
    "c = np.array(c)\n",
    "df_c[\"AGE_GROUP\"] = c\n",
    "#print(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "1d63e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.columns = (df_c.columns.str.strip().str.upper()\n",
    "              .str.replace(' ', ''))\n",
    "#print(df_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10fd38",
   "metadata": {},
   "source": [
    "# Divding test data based on Age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "f55d49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 75\n",
    "dfc_train = df_c.head(int(len(df_c)*(n/100)))\n",
    "l_train = len(dfc_train)\n",
    "grouped = dfc_train.groupby(df_c[\"AGE_GROUP\"])\n",
    "dfc_train0 = grouped.get_group(0)\n",
    "dfc_train1 = grouped.get_group(1)\n",
    "dfc_train2 = grouped.get_group(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c6ad6",
   "metadata": {},
   "source": [
    "# Separating test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "948e90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "dfc_test = df_c.tail(int(len(df_c)*(n/100)))\n",
    "age = dfc_test['AGE_GROUP']\n",
    "age = np.array(age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ffb06f",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "d6cab1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(p0, p1, p2):\n",
    "    a = max(p0, p1, p2)\n",
    "    if(a == p0):\n",
    "        return 0\n",
    "    elif(a == p1):\n",
    "        return 1\n",
    "    elif(a == p2):\n",
    "        return 2\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25532d",
   "metadata": {},
   "source": [
    "# Naive bayes Model (Univariate) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57824fe",
   "metadata": {},
   "source": [
    "In univariate models there is only one variate.  \n",
    "In Naive Bayes Model we try to fit a Guassian.  \n",
    "We fit a guassian for every class and calculate respective (sigma, u) with maximim likelihood estimation.  \n",
    "Bayers Classifier :  \n",
    "p(C = Ck |x) = (p(x|C = Ck) * p(C = CK))/P(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "4136df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_byers(data):# function to calculate u and sigma\n",
    "    u = 0\n",
    "    sigma = 0\n",
    "    for i in data:\n",
    "        u = u + i\n",
    "    u = u/len(data)\n",
    "    for i in data:\n",
    "            sigma += (i - u)*(i - u)\n",
    "    sigma = sigma/len(data)\n",
    "    sigma = math.sqrt(sigma)\n",
    "    return u,sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "07c24457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7723499361430396\n",
      "0.20306513409961685\n",
      "0.02458492975734355\n"
     ]
    }
   ],
   "source": [
    "#p(C = Ck) for all three classes\n",
    "p_0 = len(dfc_train0)/l_train\n",
    "p_1 = len(dfc_train1)/l_train\n",
    "p_2 = len(dfc_train2)/l_train\n",
    "\n",
    "print(p_0)\n",
    "print(p_1)\n",
    "print(p_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "6019fe03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73453721 0.4650852 ]\n",
      "[1.11706682 0.46660689]\n",
      "[1.23894805 0.42019658]\n"
     ]
    }
   ],
   "source": [
    "#u, sigma for all three classes\n",
    "p_x_0 = np.array(naive_byers(dfc_train0['WHOLEWEIGHT']))\n",
    "p_x_1 = np.array(naive_byers(dfc_train1['WHOLEWEIGHT']))\n",
    "p_x_2 = np.array(naive_byers(dfc_train2['WHOLEWEIGHT']))\n",
    "\n",
    "print(p_x_0)\n",
    "print(p_x_1)\n",
    "print(p_x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "d02ba807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probability p(x) with u,sigma\n",
    "def probability(u, sigma, x):\n",
    "    a = ((-1)*(x-u)*(x-u))/(2*sigma*sigma)\n",
    "    p = (1/(sigma*math.sqrt(2*math.pi)))*(math.exp(a))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f32792",
   "metadata": {},
   "source": [
    "We have u, sigma for all the classes.  \n",
    "We now use test data to predict age group.  \n",
    "For this we calculate p(C = Ck|x) for all the three classes and take maximum of all the three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "13654a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "age_predict_group =[]\n",
    "for ind in dfc_test.index:\n",
    "    p_0_x = p_0 * probability(p_x_0[0], p_x_0[1], dfc_test['WHOLEWEIGHT'][ind])\n",
    "    p_1_x = p_1 * probability(p_x_1[0], p_x_1[1], dfc_test['WHOLEWEIGHT'][ind])\n",
    "    p_2_x = p_2 * probability(p_x_2[0], p_x_2[1], dfc_test['WHOLEWEIGHT'][ind])\n",
    "    age_predict_group.append(classifier(p_0_x, p_1_x, p_2_x))  \n",
    "\n",
    "print(age_predict_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2bbf2",
   "metadata": {},
   "source": [
    "Accuracy for test data to predict the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "43686075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.90421455938697\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "count = 0\n",
    "for i in range(len(age_predict_group)):\n",
    "    if(age_predict_group[i] == age[i]):\n",
    "        count += 1\n",
    "accuracy = (count/len(age_predict_group))*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3ae51d",
   "metadata": {},
   "source": [
    "Our accuracy is 74.90421455938697.  \n",
    "Observation : Gives good results for large and diverse data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d44e8f",
   "metadata": {},
   "source": [
    "# Multivariate Guassian\n",
    "I tried few multivariate guassians and among them all Viscera weight, Whole weight and Height gave better results.  \n",
    "and also in logistic regression when taken these three variates better results were obtained.  \n",
    "So we can assume these are good variates to estimate and produces high Accuracy in Naive Bayes also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dfa1f",
   "metadata": {},
   "source": [
    "# Dividing training data based on groups and storing in matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "d25cde4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = dfc_train0.iloc[0:,0:10]\n",
    "x_0['11'] = 1\n",
    "x_0 = np.array(x_0)\n",
    "\n",
    "y_0 = dfc_train0['AGE_GROUP']\n",
    "y_0 = np.array(y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "e68d5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = dfc_train1.iloc[0:,0:10]\n",
    "x_1['11'] = 1\n",
    "x_1 = np.array(x_1)\n",
    "\n",
    "y_1 = dfc_train1['AGE_GROUP']\n",
    "y_1 = np.array(y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "386c2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = dfc_train2.iloc[0:,0:10]\n",
    "x_2['11'] = 1\n",
    "x_2 = np.array(x_2)\n",
    "\n",
    "y_2 = dfc_train2['AGE_GROUP']\n",
    "y_2 = np.array(y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d286c",
   "metadata": {},
   "source": [
    "# Logistic Regression (Gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c7d2d",
   "metadata": {},
   "source": [
    "In logistic Regression we try to select a curve which gives better results.  \n",
    "p(C = Ck |x, w) = softmax(W.T @ X)  \n",
    "To find weights we try to maximize probability with maximimum likelihood estimation using gradient descent.    \n",
    "Gradient descent : Θ(new) = Θ(old) - learingrate * d/dΘ(loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "3d00659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, w):\n",
    "    a = np.exp(x@w_random_0)/ (np.exp(x_0@w_random_0) + np.exp(x_0@w_random_1) + np.exp(x_0@w_random_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "4402e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_logistic = 0.0000001\n",
    "num_iters_logistic = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "23e85cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00242963 0.00188144 0.00063493 0.00354732 0.00159932 0.00077897\n",
      " 0.00098986 0.00131374 0.00187534 0.00164119 0.00483027]\n",
      "[0.00074694 0.00058966 0.00020991 0.00142167 0.00057333 0.00030544\n",
      " 0.00043357 0.00055227 0.00012607 0.00059431 0.00127265]\n",
      "[9.33386065e-05 7.43385916e-05 2.69729331e-05 1.91113256e-04\n",
      " 6.93358376e-05 3.86521029e-05 6.32899149e-05 8.01244453e-05\n",
      " 6.00752376e-06 6.81096371e-05 1.54241606e-04]\n"
     ]
    }
   ],
   "source": [
    "##To calculate weights\n",
    "w_random_0 = np.zeros(11)\n",
    "w_random_1 = np.zeros(11)\n",
    "w_random_2 = np.zeros(11)\n",
    "y_0 = np.ones(len(x_0))\n",
    "y_1 = np.ones(len(x_1))\n",
    "y_2 = np.ones(len(x_2))\n",
    "for _ in range(num_iters_logistic):\n",
    "    y_p_0 = x_0@w_random_0\n",
    "    a0 = np.exp(x_0@w_random_0)/ (np.exp(x_0@w_random_0) + np.exp(x_0@w_random_1) + np.exp(x_0@w_random_2))\n",
    "    dJ0 = x_0.T@(a0 - y_0)\n",
    "    w_random_0 = w_random_0 - learning_rate_logistic*dJ0\n",
    "    y_p_1 = x_1@w_random_1\n",
    "    a1 = np.exp(x_1@w_random_1)/ (np.exp(x_1@w_random_0) + np.exp(x_1@w_random_1) + np.exp(x_1@w_random_2))\n",
    "    dJ1 = x_1.T@(a1 - y_1)\n",
    "    w_random_1 = w_random_1 - learning_rate_logistic*dJ1\n",
    "    y_p_2 = x_2@w_random_2\n",
    "    a2 = np.exp(x_2@w_random_2)/ (np.exp(x_2@w_random_0) + np.exp(x_2@w_random_1) + np.exp(x_2@w_random_2))\n",
    "    dJ2 = x_2.T@(a2 - y_2)\n",
    "    w_random_2 = w_random_2 - learning_rate_logistic*dJ2\n",
    "    \n",
    "print(w_random_0)\n",
    "print(w_random_1)\n",
    "print(w_random_2)\n",
    "w_random = [w_random_0, w_random_1, w_random_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f457c",
   "metadata": {},
   "source": [
    "Testing for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "35a367e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_l = dfc_test['AGE_GROUP']\n",
    "age_l = np.array(age_l)\n",
    "x_test = dfc_test.iloc[0:,0:10]\n",
    "x_test['11'] = 1\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "7bbedb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(weight, x):\n",
    "    sum = 0\n",
    "    for i in range(3):\n",
    "        sum += np.exp(weight[i].T@x)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "4c8cb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "agegroup_logistic = []\n",
    "i = 0\n",
    "for i in range(len(x_test)):\n",
    "    p_0_xl = np.exp(w_random_0.T@x_test[i])/sum(w_random, x_test[i])\n",
    "    p_1_xl = np.exp(w_random_1.T@x_test[i])/sum(w_random, x_test[i])\n",
    "    p_2_xl = np.exp(w_random_1.T@x_test[i])/sum(w_random, x_test[i])\n",
    "    agegroup_logistic .append(classifier(p_0_xl, p_1_xl, p_2_xl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "78409f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.34099616858238\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "count_l = 0\n",
    "for i in range(len(agegroup_logistic)):\n",
    "    if(agegroup_logistic[i] == age_l[i]):\n",
    "        count_l += 1\n",
    "accuracy_l = (count_l/len(agegroup_logistic))*100\n",
    "print(accuracy_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5863125",
   "metadata": {},
   "source": [
    "Accuracy is 76.34099616858238\n",
    "Observation : Accuracy for logistic Regression(gradient descent) is more than Naive Bayes Model(univariate).  \n",
    "We can say that model gives better results for multivariate  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9edc1",
   "metadata": {},
   "source": [
    "# Logistic Regression (Newton Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af527cbc",
   "metadata": {},
   "source": [
    "In logistic Regression we try to select a curve which gives better results.  \n",
    "p(C = Ck |x, w) = softmax(W.T @ X)  \n",
    "To find weights we try to maximize probability with maximimum likelihood estimation using newton method.  \n",
    "Θ(new) = Θ(old) - H^(-1)@G  \n",
    "G is gradient.   \n",
    "G = X.T@(Y_pred - Y)  \n",
    "H is Hessian matrix  \n",
    "H = X.T @ S @ X  \n",
    "S = Y_pred(1 - Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "b26cf101",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_logistic = 0.01\n",
    "num_iters_logistic = 10\n",
    "correction = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "92dfa81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate weights \n",
    "def gradient_descent_logistic_newton(data_0, data_1, data_2, learning_rate, weight, num_iters, correction):\n",
    "    i, j, k = 0, 0, 0\n",
    "    for i in range(num_iters):\n",
    "        gra = [[0 for i in range(11)] for j in range(3)]\n",
    "        gra = np.array(gra , dtype = float)\n",
    "        s_0 = np.zeros(len(data_0))\n",
    "        s_1 = np.zeros(len(data_1))\n",
    "        s_2 = np.zeros(len(data_2))\n",
    "        for j in range(len(data_0)):\n",
    "            for k in range(11):\n",
    "                a = np.exp((data_0[j]*weight[0]).sum())\n",
    "                #print(a)\n",
    "                b = np.exp((data_0[j]*weight[0]).sum()) + np.exp((data_0[j]*weight[1]).sum()) + np.exp((data_0[j]*weight[2]).sum())\n",
    "                gra[0][k] += (a/b - 1)*data_0[j][k]\n",
    "                m = ((a/b) - (a/b)*(a/b))\n",
    "            s_0[j] = m\n",
    "        s0 = np.diag(s_0)\n",
    "        h_0 = data_0.T @ s0 @ data_0\n",
    "        for j in range(len(data_1)):\n",
    "            for k in range(11):\n",
    "                a = np.exp((data_1[j]*weight[1]).sum())\n",
    "                b = np.exp((data_1[j]*weight[0]).sum()) + np.exp((data_1[j]*weight[1]).sum()) + np.exp((data_1[j]*weight[2]).sum())\n",
    "                gra[1][k] += (a/b - 1)*data_1[j][k]\n",
    "                m = ((a/b) - (a/b)*(a/b))\n",
    "            s_1[j] = m\n",
    "        s1 = np.diag(s_1)\n",
    "        h_1 = data_1.T @ s1 @ data_1\n",
    "        for j in range(len(data_2)):\n",
    "            for k in range(11):\n",
    "                a = np.exp((data_2[j]*weight[2]).sum())\n",
    "                b = np.exp((data_2[j]*weight[0]).sum()) + np.exp((data_2[j]*weight[1]).sum()) + np.exp((data_2[j]*weight[2]).sum())\n",
    "                gra[2][k] += (a/b - 1)*data_2[j][k]\n",
    "                m = ((a/b) - (a/b)*(a/b))\n",
    "            s_2[j] = m\n",
    "        s2 = np.diag(s_2)\n",
    "        h_2 = data_2.T @ s2 @ data_2        \n",
    "        weight[0] = weight[0] - np.linalg.inv(h_0 - correction * np.identity(11))@gra[0]\n",
    "        weight[1] = weight[1] - np.linalg.inv(h_1 - correction * np.identity(11))@gra[1]\n",
    "        weight[2] = weight[2] - np.linalg.inv(h_2 - correction * np.identity(11))@gra[2]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "55664634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.14582321e-02 -1.31057767e-02 -5.07227149e-02  7.78157965e-03\n",
      "   1.67328498e-02  5.26116704e-03  1.78978778e-03  7.51324614e+00\n",
      "   7.51121086e+00  7.51364238e+00  2.25380948e+01]\n",
      " [-7.15341241e-02 -7.20992917e-03 -3.76225059e-02  6.63163460e-03\n",
      "   1.41017907e-02  5.86397831e-04 -1.97832385e-03  7.51021975e+00\n",
      "   7.50830874e+00  7.51076776e+00  2.25292956e+01]\n",
      " [ 1.52447363e-01  1.79595503e-02  8.47607885e-02 -1.35718713e-02\n",
      "  -2.90377752e-02 -5.04493293e-03  9.59243222e-04  7.47804239e+00\n",
      "   7.48177621e+00  7.47713670e+00  2.24369553e+01]]\n"
     ]
    }
   ],
   "source": [
    "weight_newton = [[0 for i in range(11)] for j in range(3)]\n",
    "weight_newton = np.array(weight_newton, dtype = np.float128)\n",
    "weight_newton = gradient_descent_logistic_newton(x_0, x_1, x_2, learning_rate_logistic, weight_newton, num_iters_logistic, correction)\n",
    "print(weight_newton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "61151235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 2, 0, 2, 2, 2, 0, 1, 1, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 1, 0, 1, 2, 0, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 0, 2, 0, 2, 2, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 2, 0, 0, 0, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 0, 2, 1, 2, 2, 2, 1, 2, 2, 1, 0, 1, 2, 1, 2, 1, 1, 1, 0, 1, 1, 2, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 2, 0, 1, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 0, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 1, 2, 0, 2, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 2, 2, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 2, 2, 2, 2, 0, 1, 1, 0, 2, 2, 0, 2, 2, 2, 0, 1, 2, 1, 1, 1, 2, 2, 0, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 1, 1, 2, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 2, 2, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 2, 0, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 0, 2, 2, 2, 1, 0, 1, 0, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 1, 2, 1, 0, 0, 0, 0, 2, 2, 1, 0, 1, 0, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "agegroup_logistic_newton = []\n",
    "i = 0\n",
    "for i in range(len(x_test)):\n",
    "    p_0_xln = np.exp(weight_newton[0].T@x_test[i])/sum(weight_newton, x_test[i])\n",
    "    #print(p_0_xl)\n",
    "    p_1_xln = np.exp(weight_newton[1].T@x_test[i])/sum(weight_newton, x_test[i])\n",
    "    #print(p_1_xl)\n",
    "    p_2_xln = np.exp(weight_newton[2].T@x_test[i])/sum(weight_newton, x_test[i])\n",
    "    #print(p_2_xl)\n",
    "    agegroup_logistic_newton .append(classifier(p_0_xln, p_1_xln, p_2_xln))\n",
    "print(agegroup_logistic_newton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "52fac918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.46743295019157\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "count_ln = 0\n",
    "for i in range(len(agegroup_logistic_newton)):\n",
    "    if(agegroup_logistic_newton[i] == age_l[i]):\n",
    "        count_ln += 1\n",
    "accuracy_ln = (count_ln/len(agegroup_logistic_newton))*100\n",
    "print(accuracy_ln)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6565f01",
   "metadata": {},
   "source": [
    "Accuracy is 48.46743295019157  \n",
    "Which is less than both naive Bayes and logistic Regression(gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e04a93",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We got better results i.e Accuracy for Logistic Regression (Gradient descent) among the three methods we used.  \n",
    "THerefore, Logistic Regression (Gradient descent) is better model for the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
